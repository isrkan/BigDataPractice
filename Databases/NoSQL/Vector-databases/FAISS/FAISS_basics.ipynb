{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734232d1-2c42-4297-8712-ccc538ac6745",
   "metadata": {
    "id": "734232d1-2c42-4297-8712-ccc538ac6745"
   },
   "source": [
    "# FAISS basics\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It is widely used in applications involving nearest-neighbor search and large-scale machine learning. Common use cases include image search (finding similar images based on features), text search using embeddings (e.g., semantic similarity) and clustering high-dimensional data.\n",
    "\n",
    "#### Core concept: Vector similarity search\n",
    "FAISS works with dense vectors. For example, a vector can represent:\n",
    "- An image (via image embeddings).\n",
    "- A document (via text embeddings like BERT).\n",
    "\n",
    "The library finds the nearest vectors (most similar vectors) to a given query vector.\n",
    "\n",
    "#### Key components:\n",
    "1. **Index:** A data structure used to store vectors for search.\n",
    "2. **Search:** A method to retrieve the nearest vectors based on a similarity metric (e.g., Euclidean or cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4727aa72-699f-4758-a16a-a81aca2b68f9",
   "metadata": {
    "id": "4727aa72-699f-4758-a16a-a81aca2b68f9"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e3f13-93f8-4434-ac34-440cf130be84",
   "metadata": {
    "id": "500e3f13-93f8-4434-ac34-440cf130be84"
   },
   "source": [
    "#### Example dataset\n",
    "Let’s create a small dataset of random vectors to simulate how FAISS works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06f11f9-880e-4289-ae0c-9f0f53fe5d31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e06f11f9-880e-4289-ae0c-9f0f53fe5d31",
    "outputId": "398b6c2a-be87-4337-d207-0448147c6649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (1000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Generate a dataset of random vectors\n",
    "# Assume we have 1000 vectors, each of dimension 128 (common in embeddings)\n",
    "dimension = 128  # Length of each vector\n",
    "num_vectors = 1000  # Number of vectors in the dataset\n",
    "\n",
    "# Randomly generate vectors\n",
    "data = np.random.random((num_vectors, dimension)).astype('float32')  # FAISS requires float32\n",
    "\n",
    "print(\"Shape of dataset:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b5783-9914-4e82-b5b0-41429954e8ce",
   "metadata": {
    "id": "706b5783-9914-4e82-b5b0-41429954e8ce"
   },
   "source": [
    "## Indexing strategies\n",
    "\n",
    "### IndexFlatL2\n",
    "FAISS provides several types of indices. The simplest one is the **IndexFlatL2**, which performs brute-force search using the L2 (Euclidean) distance. It stores all the data (vectors) in memory so that when we want to search for something, it directly compares our query with every vector in the dataset. Since it checks each vector individually, the search is guaranteed to be accurate, but it can be slower for larger datasets. IndexFlatL2 is ideal for small datasets, such as those with thousands of vectors, where precision is more important than speed.\n",
    "\n",
    "The \"flat\" in the name means that all the vectors are stored in a simple, non-hierarchical way without any advanced indexing techniques such as additional structure like clustering or partitioning. This makes the structure \"flat,\" where every vector is directly accessible and compared individually during the search.\n",
    "\n",
    "#### Step 1: Initialize the index\n",
    "Initializing an index in FAISS means creating a data structure that will store vectors for the purpose of similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bn1Mq6Xt-__c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bn1Mq6Xt-__c",
    "outputId": "551930da-48b7-47de-b376-fb4179b49b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the index trained? True\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize an index\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 indicates Euclidean distance\n",
    "print(\"Is the index trained?\", index.is_trained)  # Flat indices are always trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X7DhpbIu_Aiq",
   "metadata": {
    "id": "X7DhpbIu_Aiq"
   },
   "source": [
    "In this case, we initialize an index that uses the L2 (Euclidean) distance to measure similarity. The `dimension` specifies the length of each vector, so the index knows what size each vector should be.\n",
    "\n",
    "In FAISS, training refers to the process of preparing an index to be used for efficient search. Some types of indices require training on a sample of data to learn how to organize the vectors for fast search. However, for simple indices like IndexFlatL2, which simply store the vectors in memory and perform a brute-force search, training is not required.\n",
    "\n",
    "#### Step 2: Add vectors to the index\n",
    "When we add vectors to the index in FAISS, we are essentially storing the vectors (e.g., image embeddings or text embeddings) in the index so that they can later be used for similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wVrWjb4q_A9Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVrWjb4q_A9Y",
    "outputId": "e648977f-4502-4872-89f4-2b8adf117e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the index: 1000\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Add vectors to the index\n",
    "index.add(data)  # Add the dataset to the index\n",
    "print(\"Number of vectors in the index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o4lmeI8W_Pug",
   "metadata": {
    "id": "o4lmeI8W_Pug"
   },
   "source": [
    "By adding the vectors to the index, we enable the index to perform efficient searches when we query it, comparing the query vector to those stored in the index to find the closest matches based on the similarity metric (e.g., L2 distance).\n",
    "\n",
    "#### Step 3: Perform a search\n",
    "Here, we will perform a search to find the nearest neighbors of a given query vector in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da550b8d-baf8-4f68-9afc-337513aca929",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da550b8d-baf8-4f68-9afc-337513aca929",
    "outputId": "ef2672cd-15ec-456f-c983-b8b8953573f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances to nearest neighbors: [[14.414135 15.166716 16.051807 16.167038 16.283192]]\n",
      "Indices of nearest neighbors: [[558 782 318 735 283]]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Perform a search\n",
    "# Create a random query vector (of the same dimension as the dataset)\n",
    "query_vector = np.random.random((1, dimension)).astype('float32')\n",
    "\n",
    "# Search for the top 5 nearest neighbors\n",
    "k = 5  # Number of nearest neighbors\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# Display results\n",
    "print(\"Distances to nearest neighbors:\", distances)\n",
    "print(\"Indices of nearest neighbors:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d983be7-858c-47f1-a24e-af847166d17b",
   "metadata": {
    "id": "9d983be7-858c-47f1-a24e-af847166d17b"
   },
   "source": [
    "The query vector must have the same dimension as the vectors in the dataset (in this case, 128), because the index stores vectors of that size.\n",
    "- `distances`: This variable will contain the distances (e.g., Euclidean distance) between the query vector and the 5 nearest vectors found in the index.\n",
    "- `indices`: This variable contains the indices (or positions) of the nearest vectors in the index. These indices tell us where in the dataset the closest vectors are located.\n",
    "\n",
    "### IndexIVFFlat (Inverted file index + flat search)\n",
    "IndexIVFFlat is an advanced indexing technique that improves search speed by dividing the dataset into smaller groups (clusters) using a clustering algorithm. Instead of comparing a query to every vector in the dataset, the query is compared only to vectors in the most relevant cluster(s). This reduces the number of comparisons, making it much faster for large datasets. However, it is an approximate method, so results may not be as accurate as brute-force (IndexFlatL2).\n",
    "\n",
    "#### Step 1: Create an IVF index\n",
    "Here, we are setting up an indexing structure that partitions the dataset into smaller groups to make searching faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "_h0sGdkyLmXW",
   "metadata": {
    "id": "_h0sGdkyLmXW"
   },
   "outputs": [],
   "source": [
    "# Step 1: Create an IVF index\n",
    "nlist = 100  # Number of clusters (partitions)\n",
    "quantizer = faiss.IndexFlatL2(dimension)  # Quantizer for clustering\n",
    "ivf_index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ax9B7XZNW0w",
   "metadata": {
    "id": "6ax9B7XZNW0w"
   },
   "source": [
    "The choice of the number of clusters depends on how large the dataset is and the balance between speed and accuracy. A larger number of clusters can improve accuracy but might increase memory usage and search time. A smaller number of clusters makes the index more efficient but could lead to less accurate results.\n",
    "\n",
    "The quantizer is a technique used to perform clustering. In this case, `faiss.IndexFlatL2(dimension)` is used as the quantizer, which means the vectors will be grouped based on L2 (Euclidean) distance. The quantizer helps FAISS group similar vectors together by measuring how close the vectors are to each other.\n",
    "\n",
    "#### Step 2: Train the index\n",
    "Training the index is an essential step when using an IVF index, as it learns how to organize data into clusters. To do this, it needs a set of representative data — a sample of the dataset that reflects the general distribution of the vectors, so that the index can effectively divide the vectors into meaningful clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "_9zyLF-tLmz1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9zyLF-tLmz1",
    "outputId": "ba5452c7-14b6-42b9-8029-4ed22b5c42fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the IVF index trained? True\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train the index\n",
    "ivf_index.train(data)  # Training requires representative data\n",
    "print(\"Is the IVF index trained?\", ivf_index.is_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YGYguxbWPile",
   "metadata": {
    "id": "YGYguxbWPile"
   },
   "source": [
    "During training, the IVF index uses the training data (in this case, `data`) to find the best way to partition the vector space into clusters. When we call `ivf_index.train(data)`, FAISS uses the k-means algorithm to analyze the data and figure out how to split it into clusters. After training, the IVF index will store these centroids (representing each cluster), and when we perform a search, the query is first compared to the centroids of the clusters.\n",
    "\n",
    "#### Step 3: Add vectors to the index\n",
    "In this step, the vectors are added to the IVF index that we have created and trained. The IVF index does not store vectors in a flat structure like IndexFlatL2. Instead, it organizes them into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "l3HilSW9LnCB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3HilSW9LnCB",
    "outputId": "bef68a5d-14f5-48ad-e368-9d0412b6183b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the IVF index: 1000\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Add vectors to the index\n",
    "ivf_index.add(data)\n",
    "print(\"Number of vectors in the IVF index:\", ivf_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2fhFIgfRN4y",
   "metadata": {
    "id": "P2fhFIgfRN4y"
   },
   "source": [
    "The vectors that are added are placed into the appropriate clusters that were determined during the training step (they are assigned to the closest cluster centroid). After this step, the IVF index will have access to the vectors, and it can perform similarity searches on them.\n",
    "\n",
    "### Step 4: Perform a search\n",
    "In this step, we use the trained IVF index to perform a similarity search for a given query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "S_qO0252LnN7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_qO0252LnN7",
    "outputId": "35f7cdc6-b150-421e-87f0-354205e6d63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVF Distances to nearest neighbors: [[16.167038 16.741135 16.834538 17.297047 17.438253]]\n",
      "IVF Indices of nearest neighbors: [[735 697 421 995 593]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Perform a search\n",
    "distances, indices = ivf_index.search(query_vector, k)  # Search for top-k neighbors\n",
    "print(\"IVF Distances to nearest neighbors:\", distances)\n",
    "print(\"IVF Indices of nearest neighbors:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roWg1Z6BSfYe",
   "metadata": {
    "id": "roWg1Z6BSfYe"
   },
   "source": [
    "Here, we search for the top-k nearest neighbors to the query_vector in the IVF index. The query vector is first compared to the centroids of the clusters in the IVF index. Then, the query is only compared to vectors in the most relevant clusters and the top-k most similar vectors (neighbors) are found based on a similarity metric (e.g., Euclidean distance).\n",
    "- `distances` shows the closeness of the query vector to the neighbors.\n",
    "- `indices` shows the actual positions of the neighbors in the original dataset.\n",
    "\n",
    "### IndexIVFPQ (Inverted file index + product quantization)\n",
    "IndexIVFPQ combines the clustering of IndexIVFFlat with product quantization (PQ), a compression technique that reduces the memory footprint of each vector. PQ breaks each vector into smaller chunks and compresses them into smaller representations. This method is highly memory-efficient and faster, though it introduces more approximation.\n",
    "\n",
    "In IVF-PQ, vectors are divided into clusters (using IVF), and each vector within a cluster is compressed (using PQ) to reduce memory usage. PQ works by breaking each high-dimensional vector into smaller parts and representing them with fewer bits, thus reducing the storage needed for each vector.\n",
    "\n",
    "#### Step 1: Create an IVF-PQ index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tc3xzoTKSGjK",
   "metadata": {
    "id": "tc3xzoTKSGjK"
   },
   "outputs": [],
   "source": [
    "# Step 1: Create an IVF-PQ index\n",
    "m = 8  # Number of chunks for PQ\n",
    "ivfpq_index = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, 8)  # (m, 8) -> PQ compression level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ImbkXSlYT6q7",
   "metadata": {
    "id": "ImbkXSlYT6q7"
   },
   "source": [
    "`m` is the number of chunks that the vector will be divided into during the PQ process. For example, if we have a vector of dimension 128 and set `m=8`, each vector will be divided into 8 chunks, with each chunk representing a part of the original vector. More chunks (`m`) means more compression, but each chunk will carry less information (and the approximation will be higher).\n",
    "\n",
    "The second `8` in the constructor is the quantization level (the number of bits used to represent each chunk) and it determines how much memory is saved. Typically, a smaller number of bits for each chunk means higher compression but more approximation, as the vector will be represented less precisely. For example, `8` means each chunk will be represented with 8 bits, while a larger number might mean less compression (but more accuracy).\n",
    "\n",
    "#### Step 2: Train the index\n",
    "In this step, we are training the IVF-PQ index on the provided dataset. The purpose of this training is to prepare the index for efficient similarity searches, involving both clustering (IVF) and compression (PQ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "YOZTTF46T6-H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOZTTF46T6-H",
    "outputId": "7c1d6a06-da45-482e-abb8-f46f8ceead0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the IVFPQ index trained? True\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train the index\n",
    "ivfpq_index.train(data)  # Training for clustering and PQ compression\n",
    "print(\"Is the IVFPQ index trained?\", ivfpq_index.is_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DrJLNOsrT7VE",
   "metadata": {
    "id": "DrJLNOsrT7VE"
   },
   "source": [
    "During the training, the IVF part of the index learns to divide the dataset into clusters. In addition, the training process also learns how to compress the vectors using PQ. The PQ process divides each vector into chunks (as defined by `m`), and it calculates how to compress each chunk efficiently. Clustering (IVF) and compression (PQ) require training because they need to understand the structure of the data (how the vectors are distributed and how they can be compressed).\n",
    "\n",
    "#### Step 3: Add vectors to the index\n",
    "After training the index, the next step is to add the actual data vectors into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57QlbQB_SGld",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57QlbQB_SGld",
    "outputId": "28646b83-21c2-4a0a-e521-39319964c081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the IVFPQ index: 1000\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Add vectors to the index\n",
    "ivfpq_index.add(data)\n",
    "print(\"Number of vectors in the IVFPQ index:\", ivfpq_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rhk-QybKUKGR",
   "metadata": {
    "id": "Rhk-QybKUKGR"
   },
   "source": [
    "Each vector will be placed into its appropriate cluster (determined during training) based on which cluster centroid is closest. PQ is used to compress each vector as it’s added to the index, reducing memory usage, as was determined during the training phase.\n",
    "\n",
    "#### Step 4: Perform a search\n",
    "In this step, we are performing a search on the IVF-PQ index to find the nearest neighbors to a given query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "xbD1H-19UKcp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbD1H-19UKcp",
    "outputId": "0afd5db5-2582-4471-9b11-40932bbc9990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVFPQ Distances to nearest neighbors: [[12.494565  13.5019045 13.937946  14.385806  14.43169  ]]\n",
      "IVFPQ Indices of nearest neighbors: [[735 995 593 203 280]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Perform a search\n",
    "distances, indices = ivfpq_index.search(query_vector, k)  # Search for top-k neighbors\n",
    "print(\"IVFPQ Distances to nearest neighbors:\", distances)\n",
    "print(\"IVFPQ Indices of nearest neighbors:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SZNWSlGGymdl",
   "metadata": {
    "id": "SZNWSlGGymdl"
   },
   "source": [
    "First, the search checks which cluster(s) the query vector likely belongs to (based on the trained centroids from the IVF step). Since the index is using PQ, each vector is stored in a compressed form. During the search, these compressed representations are used.\n",
    "\n",
    "### IndexScalarQuantizer (Scalar quantization)\n",
    "IndexScalarQuantizer compresses vectors using scalar quantization. Each vector dimension is quantized (discretized) into a small number of values (quantization levels), reducing memory usage. Unlike PQ, which compresses chunks of dimensions, scalar quantization operates dimension by dimension. This is simpler and less memory-efficient than PQ, but it can still speed up searches while saving memory.\n",
    "\n",
    "Instead of using the exact values for each dimension, scalar quantization maps each dimension to a fixed number of levels (like rounding to a specific number of values).For example, if we have a vector with values like `[0.2, 0.5, 0.8]`, scalar quantization might map each value to a small set of possible values, such as `[0, 0.5, 1]`. This process reduces the precision of the vector, saving memory.\n",
    "\n",
    "#### Step 1: Create a ScalarQuantizer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8TtVueU6zi8i",
   "metadata": {
    "id": "8TtVueU6zi8i"
   },
   "outputs": [],
   "source": [
    "# Step 1: Create a ScalarQuantizer index\n",
    "scalar_quantizer = faiss.ScalarQuantizer(dimension, faiss.ScalarQuantizer.QT_8bit)  # 8-bit quantization\n",
    "sq_index = faiss.IndexIVFScalarQuantizer(quantizer, dimension, nlist, scalar_quantizer.qtype, faiss.METRIC_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vii-_zt4zjNw",
   "metadata": {
    "id": "vii-_zt4zjNw"
   },
   "source": [
    "The `faiss.ScalarQuantizer.QT_8bit` specifies that each dimension will be quantized to 8 bits, meaning each dimension can take one of 256 possible values (2^8).\n",
    "\n",
    "#### Step 2: Train the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "KP57amJQzjgG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KP57amJQzjgG",
    "outputId": "2fb7e55c-1f7c-4071-a056-92c9757b6562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the ScalarQuantizer index trained? True\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train the index\n",
    "sq_index.train(data)\n",
    "print(\"Is the ScalarQuantizer index trained?\", sq_index.is_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AQqABY99zjub",
   "metadata": {
    "id": "AQqABY99zjub"
   },
   "source": [
    "#### Step 3: Add vectors to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "-sJNS2G6zlpQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sJNS2G6zlpQ",
    "outputId": "967331f4-47c9-4046-a8c7-63167ea82ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the ScalarQuantizer index: 1000\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Add vectors to the index\n",
    "sq_index.add(data)\n",
    "print(\"Number of vectors in the ScalarQuantizer index:\", sq_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V7eZx-ICzj-c",
   "metadata": {
    "id": "V7eZx-ICzj-c"
   },
   "source": [
    "#### Step 4: Perform a search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8LSQ9DKsSGo1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LSQ9DKsSGo1",
    "outputId": "25d46682-fd1d-4f38-a190-bd79122395d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarQuantizer Distances to nearest neighbors: [[16.180897 16.732254 16.82972  17.307941 17.425316]]\n",
      "ScalarQuantizer Indices of nearest neighbors: [[735 697 421 995 593]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Perform a search\n",
    "distances, indices = sq_index.search(query_vector, k)  # Search for top-k neighbors\n",
    "print(\"ScalarQuantizer Distances to nearest neighbors:\", distances)\n",
    "print(\"ScalarQuantizer Indices of nearest neighbors:\", indices)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
