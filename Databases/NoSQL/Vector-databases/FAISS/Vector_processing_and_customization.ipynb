{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFGPQrRHLFa3"
   },
   "source": [
    "# **Vector processing and customization in FAISS**\n",
    "\n",
    "FAISS is a powerful library for nearest-neighbor search and clustering. While its core functionality revolves around indexing and searching vectors, FAISS also provides tools for advanced **vector processing** and **customization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KsSh3v3xLEQ4"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xWAemlBLcLQ"
   },
   "source": [
    "## Vector normalization\n",
    "Normalization scales vectors to have a unit length (magnitude of 1). This is especially useful for metrics like cosine similarity, where the direction of the vector matters more than its magnitude. We normalize vectors:\n",
    "- Equal weight in similarity computations: If we don't normalize, vectors of different lengths might dominate the similarity computation, even if their directions are similar.\n",
    "- Cosine similarity: In particular, for cosine similarity, we care only about the angle (or direction) between vectors, not their length. So, by normalizing the vectors, we make sure that all vectors are treated equally in terms of their length.\n",
    "\n",
    "FAISS provides a built-in function `normalize_L2` to scale vectors for L2 norm (magnitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aZPT5yoLETR",
    "outputId": "e2987579-8df9-4f28-a672-4b35872a0365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of the first 5 vectors after normalization: [1.         1.         0.99999994 1.0000001  0.99999994]\n"
     ]
    }
   ],
   "source": [
    "# Generate random vectors\n",
    "dimension = 128  # Length of each vector\n",
    "num_vectors = 1000  # Total number of vectors\n",
    "data = np.random.random((num_vectors, dimension)).astype('float32')\n",
    "\n",
    "# Normalize the dataset\n",
    "faiss.normalize_L2(data)\n",
    "\n",
    "# Check the norms of the vectors (should be close to 1)\n",
    "norms = np.linalg.norm(data, axis=1)\n",
    "print(\"Norm of the first 5 vectors after normalization:\", norms[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzWJ4QUOMCz5"
   },
   "source": [
    "Here, we create 1000 vectors, each with 128 dimensions. These vectors are randomly generated. Then, we scale all the vectors so that each vector has a length of 1. After this step, each vector's magnitude (or norm) will be 1. And then, we compute the magnitude (norm) of each vector and prints the first 5.\n",
    "\n",
    "#### Using normalized vectors for cosine similarity\n",
    "Cosine similarity is used to measure the angle between two vectors. By normalizing the vectors, we convert cosine similarity into inner product (dot product) between the vectors, which is easier to compute. We can use IndexFlatIP (inner product) to compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoOhFB8vLEVv",
    "outputId": "85156d00-5d0b-4fa9-b340-99c32d381984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors: [[828 527 986 333 791]]\n",
      "Cosine similarity scores: [[0.84198797 0.83181256 0.82972383 0.8245963  0.82215536]]\n"
     ]
    }
   ],
   "source": [
    "# Create an index for inner product search\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# Add normalized vectors to the index\n",
    "index.add(data)\n",
    "\n",
    "# Create a query vector and normalize it\n",
    "query_vector = np.random.random((1, dimension)).astype('float32')\n",
    "faiss.normalize_L2(query_vector)\n",
    "\n",
    "# Perform a search\n",
    "k = 5\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Cosine similarity scores:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjHwthHNOrbk"
   },
   "source": [
    "We create an index using inner product (dot product) as the similarity measure. After normalizing the vectors, the inner product is the same as cosine similarity, because the vectors have unit length. Then, we add our normalized vectors to the index so that we can search for similar vectors in this dataset. Later, we generate a query vector and normalize it in the same way as the dataset, and perform a search to find the top 5 nearest neighbors to our query vector, using cosine similarity (since the vectors and query are normalized).\n",
    "\n",
    "The `distances` returned will represent the similarity scores (higher is more similar), and the `indices` represent the positions of the nearest neighbors in the dataset.\n",
    "\n",
    "## Custom distance metrics\n",
    "Sometimes, the default similarity metrics like L2 distance (Euclidean) or inner product aren't sufficient. We might need a custom metric, such as:\n",
    "- Weighted Euclidean distance.\n",
    "- Manhattan distance.\n",
    "- Pre-processed distances (e.g., scaling vector components).\n",
    "\n",
    "Custom metrics allow us to tailor similarity computations to specific applications, such as domain-specific weighting of vector dimensions. FAISS doesnâ€™t directly support custom distance metrics, but we can preprocess vectors before adding them to the index.\n",
    "\n",
    "#### Example: Weighted Euclidean distance\n",
    "Apply a weight vector to scale dimensions before indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoU6BUl3LEYL",
    "outputId": "f6c724c3-13b0-4495-8cea-8b24d8677a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances with weighted Euclidean metric: [[0.09643585 0.09670793 0.09840918 0.10057575 0.1029233 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define a weight vector\n",
    "weights = np.random.random(dimension).astype('float32')\n",
    "\n",
    "# Preprocess data: Apply weights\n",
    "weighted_data = data * weights\n",
    "\n",
    "# Index the weighted data\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(weighted_data)\n",
    "\n",
    "# Preprocess query vector and search\n",
    "weighted_query = query_vector * weights\n",
    "distances, indices = index.search(weighted_query, k)\n",
    "\n",
    "print(\"Distances with weighted Euclidean metric:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxb7RzEgRv0F"
   },
   "source": [
    "#### Example: Manhattan distance (L1 distance)\n",
    "Since FAISS does not directly support Manhattan distance, we can:\n",
    "1. Preprocess the data: Compute distances manually after retrieving nearest neighbors based on another metric (like L2 distance).\n",
    "2. Postprocess results: Re-rank the results based on the Manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4R-50d2LEao",
    "outputId": "80a63138-f367-43e2-9ad2-c33a2e363136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original indices (by L2): [828 527 986 333 791 317 504 201 132 826]\n",
      "Re-ranked indices (by Manhattan): [828 986 527 826 317 201 791 333 504 132]\n",
      "Manhattan distances: [4.9906335 5.201288  5.263769  5.328466  5.367114  5.3852134 5.400983\n",
      " 5.4567366 5.4767303 5.5304623]\n"
     ]
    }
   ],
   "source": [
    "# Create an L2 index (FAISS does not support Manhattan distance directly)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(data)\n",
    "\n",
    "# Perform a preliminary search with L2 (to reduce the candidate set)\n",
    "k = 10  # Top-k neighbors to retrieve\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# Postprocess: Calculate Manhattan distance manually for top candidates\n",
    "def compute_manhattan_distance(query, candidates):\n",
    "    return np.sum(np.abs(candidates - query), axis=1)\n",
    "\n",
    "# Retrieve the candidate vectors\n",
    "candidate_vectors = data[indices[0]]\n",
    "\n",
    "# Compute Manhattan distances for the candidates\n",
    "manhattan_distances = compute_manhattan_distance(query_vector, candidate_vectors)\n",
    "\n",
    "# Re-rank by Manhattan distance\n",
    "manhattan_sorted_indices = np.argsort(manhattan_distances)\n",
    "\n",
    "# Display the sorted results\n",
    "print(\"Original indices (by L2):\", indices[0])\n",
    "print(\"Re-ranked indices (by Manhattan):\", indices[0][manhattan_sorted_indices])\n",
    "print(\"Manhattan distances:\", manhattan_distances[manhattan_sorted_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hvw1Rc0SMyB"
   },
   "source": [
    "FAISS is optimized for L2 and inner product similarity. Computing the Manhattan distance for all vectors in large datasets might not be efficient.\n",
    "\n",
    "## Vector transformation\n",
    "Transformations modify vectors to:\n",
    "- Reduce dimensionality (e.g., PCA).\n",
    "- Optimize the vector space for quantization or search efficiency (e.g., OPQ).\n",
    "\n",
    "We transform vectors because:\n",
    "- PCA: Reduce storage and computation costs by lowering dimensions while retaining most of the variance.\n",
    "- OPQ: Rotate and reorder vector components for better quantization and search performance.\n",
    "\n",
    "### Principal component analysis (PCA)\n",
    "PCA reduces the dimensionality of vectors by projecting them onto a lower-dimensional space while retaining the most variance. FAISS provides a `PCAMatrix` object to transform vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLKMZr_HLEdU",
    "outputId": "545d1cd0-5122-499f-f990-2c790c6b801a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1000, 128)\n",
      "Reduced shape: (1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Define a PCA matrix to reduce dimensionality to 50\n",
    "pca = faiss.PCAMatrix(dimension, 50)\n",
    "pca.train(data)  # Train PCA on the dataset\n",
    "\n",
    "# Transform the dataset\n",
    "reduced_data = pca.apply_py(data)\n",
    "print(\"Original shape:\", data.shape)\n",
    "print(\"Reduced shape:\", reduced_data.shape)\n",
    "\n",
    "# Use reduced data in an index\n",
    "index = faiss.IndexFlatL2(50)\n",
    "index.add(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwpayJdPTTgs"
   },
   "source": [
    "### Optimized product quantization (OPQ)\n",
    "OPQ is an advanced technique used to improve product quantization (PQ). It focuses on transforming the original vector space to a new space where the vectors are easier to quantize (compress) while maintaining the search accuracy.\n",
    "\n",
    "PQ splits a vector into smaller subspaces and quantizes each subspace individually. This method helps reduce memory usage, but it can sometimes lead to errors due to how the vector is split into subspaces. OPQ improves PQ by first rotating or reordering the vector components in a way that makes the quantization process more effective. It rotates vectors before applying product quantization. This optimization ensures that the subspaces are better aligned, which minimizes quantization errors and achieve better trade-offs between memory usage and search accuracy. FAISS provides an `OPQMatrix` for optimized transformation.\n",
    "\n",
    "##### Step 1: Define and train the OPQ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BkDx_B6gVN-A"
   },
   "outputs": [],
   "source": [
    "# Define an OPQ matrix with 4 subspaces\n",
    "opq = faiss.OPQMatrix(dimension, 4)\n",
    "opq.train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "079BSotGVOaK"
   },
   "source": [
    "First, we create an OPQ transformation matrix for the vectors. The matrix will divide the original vectors into 4 subspaces (the number `4` is arbitrary and can be adjusted based on the dataset size and the desired trade-off between accuracy and compression). We train the OPQ matrix on the dataset (`data`). Training the OPQ matrix means that the algorithm learns how to rotate and reorder the vectors in a way that improves the performance of the subsequent product quantization step.\n",
    "\n",
    "##### Step 2: Apply OPQ transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kv_HKUyPVZS7"
   },
   "outputs": [],
   "source": [
    "# Apply OPQ transformation\n",
    "transformed_data = opq.apply_py(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVnnWAgcVZiq"
   },
   "source": [
    "After training, we apply the transformation to the dataset. This rotates and reorders the vector components, optimizing them for better quantization and search. The result is the `transformed_data`, which will be more efficiently compressed and searched than the original vectors.\n",
    "\n",
    "##### Step 3: Define and train the product quantization (PQ) index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QYBihZubVtyl"
   },
   "outputs": [],
   "source": [
    "# Create a Product Quantization index\n",
    "pq = faiss.IndexPQ(dimension, 4, 8)  # 4 subspaces, 8 bits per subspace\n",
    "\n",
    "# Train the PQ index on the transformed data\n",
    "pq.train(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HsPwXTiVuIp"
   },
   "source": [
    "We now define a product quantization index (`IndexPQ`) on the transformed data. The number 4 represents the number of subspaces into which each vector will be split, and 8 is the number of bits used for quantization in each subspace. Then, we train the PQ index on the transformed dataset. The goal is to learn the best way to compress the data into a more memory-efficient format while still enabling effective searches.\n",
    "\n",
    "##### Step 4: Add transformed data to the PQ index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nArUVHdOWKu9"
   },
   "outputs": [],
   "source": [
    "# Add the transformed data to the PQ index\n",
    "pq.add(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IM4jKBwWK9n"
   },
   "source": [
    "After training the PQ index, we add the transformed vectors (from the OPQ step) to the index. These vectors are now ready to be searched efficiently.\n",
    "\n",
    "##### Step 5: Apply OPQ transformation to the query vector and perform the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoFhRyNfWaDu",
    "outputId": "d3a93cf9-4fc6-4eca-a34a-5d79974edb73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors: [[  7 504 349 931 260 776 440 718 415 333]]\n"
     ]
    }
   ],
   "source": [
    "# Apply OPQ transformation to the query vector\n",
    "transformed_query = opq.apply_py(query_vector)\n",
    "\n",
    "# Search with OPQ-transformed vectors\n",
    "distances, indices = pq.search(transformed_query, k)\n",
    "print(\"Indices of nearest neighbors:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIek3CMFWaS_"
   },
   "source": [
    "We apply the same OPQ transformation to the query vector. This ensures that the query vector is in the same transformed space as the data vectors, enabling a consistent and efficient search. Then, we perform a search using the transformed query vector. The `pq.search()` function looks for the top `k` nearest neighbors to the query vector, based on the product quantization representation in the transformed space.\n",
    "\n",
    "The `distances` represent the quantized similarity scores between the query vector and the nearest neighbors, and `indices` give the positions of these nearest neighbors in the index."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
