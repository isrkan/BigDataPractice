{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaNKzizmZXX0"
   },
   "source": [
    "# **Serialization and advanced indexing in FAISS**\n",
    "\n",
    "FAISS provides advanced tools for managing and optimizing indices, enabling us to:\n",
    "1. Save and load indices using serialization.\n",
    "2. Merge multiple indices for distributed or incremental indexing.\n",
    "3. Combine different index types to achieve hybrid search strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fJR-WB12ZWI6"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMqbO739Zy3l"
   },
   "source": [
    "## FAISS index serialization\n",
    "Serialization allows us to save an index to disk and reload it later. This is useful for persisting trained indices for future use, sharing indices between applications or systems and avoiding retraining on the same data.\n",
    "\n",
    "- FAISS provides functions `faiss.write_index()` and `faiss.read_index()` to save and load indices to/from a file on disk.\n",
    "- The file extension for the saved index is usually `.faiss` (or any other custom extension), and FAISS can operate on these saved indices by loading them into memory when required.\n",
    "- The index is stored in a binary format, and FAISS handles the necessary operations to ensure it can be reloaded later in exactly the same state it was when saved.\n",
    "- FAISS can store the trained index on disk, but it requires vectors to be loaded into memory during the search. This means that the vectors we add to the index need to be in-memory when performing a search. That is different from other vaector databases such as Milvus, that can handle searches without needing the entire dataset to be loaded into memory. It supports both in-memory and disk-based indexing, so the index can stay on disk while being searched efficiently through an optimized data structure (like IVF or PQ).\n",
    "\n",
    "##### Step 1: Create a random dataset and index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rDyBor77b4xl"
   },
   "outputs": [],
   "source": [
    "# Generate a dataset of random vectors\n",
    "dimension = 128  # Each vector has 128 dimensions\n",
    "num_vectors = 1000  # 1000 vectors\n",
    "data = np.random.random((num_vectors, dimension)).astype('float32')  # Generate random data\n",
    "\n",
    "# Create an index and add data\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmAFTUtqb5H9"
   },
   "source": [
    "Here, we create a dataset of 1000 random vectors, each with 128 dimensions. This data will be used for indexing and search. FAISS requires data in `float32` format for efficient processing. Then, we create an index using `IndexFlatL2`, which is a brute-force search method based on Euclidean (L2) distance. This type of index compares every vector in the dataset with the query to find the nearest neighbors. The generated dataset (`data`) is added to the index for future searches.\n",
    "\n",
    "##### Step 2: Save the index to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myXA8Ft-b5cD",
    "outputId": "9050de51-8c20-47c4-a61d-f69aadc80299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved to 'flat_index.faiss'.\n"
     ]
    }
   ],
   "source": [
    "# Save the index to disk\n",
    "faiss.write_index(index, \"flat_index.faiss\")\n",
    "print(\"Index saved to 'flat_index.faiss'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAu_lkWKb51D"
   },
   "source": [
    "We use `faiss.write_index()` to serialize the index and save it to a file called `\"flat_index.faiss\"`. This file can now be stored on disk and reloaded later.\n",
    "\n",
    "##### Step 3: Load the Index from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "don9D1K8dCva",
    "outputId": "6697c864-7d47-4a87-e132-6eaba2ec7443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded from 'flat_index.faiss'.\n"
     ]
    }
   ],
   "source": [
    "# Load the index from disk\n",
    "loaded_index = faiss.read_index(\"flat_index.faiss\")\n",
    "print(\"Index loaded from 'flat_index.faiss'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y58WBb8PdC9v"
   },
   "source": [
    "We use `faiss.read_index()` to load the saved index (`\"flat_index.faiss\"`) back into memory. The index is now ready to perform searches, exactly as it was before it was saved.\n",
    "\n",
    "##### Step 4: Perform a search on the loaded index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTpCF8bCZWp9",
    "outputId": "aaf35d41-7045-4407-a45c-acd492a10ef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: [[733 576 740 907 869]]\n"
     ]
    }
   ],
   "source": [
    "# Verify the loaded index by performing a search\n",
    "query_vector = np.random.random((1, dimension)).astype('float32')  # Create a random query vector\n",
    "distances, indices = loaded_index.search(query_vector, 5)  # Search for the top 5 nearest neighbors\n",
    "print(\"Search results:\", indices)  # Display the indices of the nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap3-24gtdl_-"
   },
   "source": [
    "After loading the index, we can use it to search for the nearest neighbors of a new `query_vector` (a random vector). We search for the top 5 nearest neighbors using the `search()` method.\n",
    "\n",
    "## Merging multiple indices\n",
    "Index merging allows us to combine two or more indices into a single index. This is useful for:\n",
    "- Distributed indexing: When data is split across multiple machines or locations, we may want to index each subset separately and later merge them into a single index for efficient searching.\n",
    "- Incremental indexing: If new vectors are added periodically, we can index the new data separately and later merge it into the main index without rebuilding the entire index from scratch.\n",
    "\n",
    "Merging indices in FAISS essentially consolidates multiple indices, each holding different subsets of data, into a single, unified index that can be used for search operations.\n",
    "\n",
    "In FAISS, indices can be merged using the `merge_from()` method. This method allows us to merge one index into another so that both indicesâ€™ data will be available in the final index. The merging process is typically done without losing any of the data, and it allows us to perform search operations on the merged index, just like we would on a single index.\n",
    "\n",
    "##### Step 1: Create two datasets with random data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FFQsxkerqBgm"
   },
   "outputs": [],
   "source": [
    "# Step 1: Create two datasets with random data\n",
    "data1 = np.random.random((500, dimension)).astype('float32')  # 500 vectors for the first dataset\n",
    "data2 = np.random.random((500, dimension)).astype('float32')  # 500 vectors for the second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MjGnDgbqBwk"
   },
   "source": [
    "We generate two separate datasets, `data1` and `data2`, each containing 500 vectors with a specified `dimension` (the number of features or elements in each vector).\n",
    "\n",
    "##### Step 2: Create two indices to hold the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kWiYhfDWqB_b"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create two indices to hold the datasets\n",
    "index1 = faiss.IndexFlatL2(dimension)  # First index for data1\n",
    "index2 = faiss.IndexFlatL2(dimension)  # Second index for data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI5fT1t_qCOt"
   },
   "source": [
    "We create two separate FAISS indices (`index1` and `index2`) using the IndexFlatL2 class.\n",
    "\n",
    "##### Step 3: Add vectors to the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eAhs0IIvqCfO"
   },
   "outputs": [],
   "source": [
    "# Step 3: Add vectors to the indices\n",
    "index1.add(data1)  # Add the first set of data to index1\n",
    "index2.add(data2)  # Add the second set of data to index2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ImRHL0ZqCtj"
   },
   "source": [
    "We add the two datasets (`data1` and `data2`) to their respective indices using the `add()` method. Each index now holds the vectors from its respective dataset.\n",
    "\n",
    "##### Step 4: Merge `index2` into `index1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-FticinrSZZ",
    "outputId": "797c0b29-6667-432a-f8ca-561a2c2bbce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors after merging: 1000\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Merge index2 into index1\n",
    "index1.merge_from(index2)\n",
    "print(\"Number of vectors after merging:\", index1.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ8MYSB0rSnu"
   },
   "source": [
    "The `merge_from()` method is called on `index1`, and `index2` is passed as an argument. This means that all the vectors in `index2` are now merged into `index1`. The result is that `index1` will now contain the combined vectors from both `data1` and `data2`. After merging, the vectors from both datasets are stored in `index1`.\n",
    "\n",
    "> Note: The indices we are merging must be of the same type. For example, we cannot merge an `IndexFlatL2` with an `IndexIVFFlat` directly. Also, merging indices that use different quantization methods is not supported directly.\n",
    "\n",
    "##### Step 5: Perform a search on the merged index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD4OfCT9rS1K",
    "outputId": "7c763b9f-0fd4-4ae9-bfa4-f7c791a2948f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: [[495 215 564 749 377]]\n"
     ]
    }
   ],
   "source": [
    "query_vector = np.random.random((1, dimension)).astype('float32')  # Create a random query vector\n",
    "distances, indices = index1.search(query_vector, 5)  # Search for the 5 nearest neighbors\n",
    "print(\"Search results:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vF5Lo_EPrTB-"
   },
   "source": [
    "We create a random query vector (`query_vector`) and search for the top 5 nearest neighbors in the merged index (`index1`).\n",
    "\n",
    "## Combining multiple indices\n",
    "Combining multiple indices involves using different index types or strategies together to create a hybrid search approach. This allows us to leverage the strengths of different indices for improved speed, accuracy, or flexibility when performing searches.\n",
    "\n",
    "##### **Why Combine Indices?**\n",
    "1. Speed vs. accuracy: Some indices, like `IndexFlatL2`, provide high accuracy but are slow, while others, like `IndexIVF` (Inverted File Index), are much faster but may introduce some approximation. Combining them allows us to get a balance of both â€” fast retrieval with good accuracy.\n",
    "2. Attribute-based partitioning: In some cases, we might want to partition data by attributes (e.g., vectors with different categories or features) and use different index types for different categories. This allows more specialized indexing for specific parts of our data.\n",
    "3. Distributed indexing: By combining indices into an `IndexShards`, we can distribute the indexing load and allow for parallel searches across multiple indices (or machines) in a scalable manner.\n",
    "\n",
    "#### Example: Combining flat and IVF indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S04nYq-husXR"
   },
   "outputs": [],
   "source": [
    "# Create a Flat index and an IVF index\n",
    "index_flat = faiss.IndexFlatL2(dimension)  # Flat index using brute-force search (L2 distance)\n",
    "index_ivf = faiss.IndexIVFFlat(faiss.IndexFlatL2(dimension), dimension, 10)  # IVF index with 10 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RamBLmeOurcC"
   },
   "source": [
    "- `index_flat` is a **brute-force index** (IndexFlatL2) that computes exact nearest neighbors using L2 (Euclidean) distance. Itâ€™s accurate but slow for large datasets.\n",
    "- `index_ivf` is an **inverted file index** (IndexIVFFlat) that uses clustering to speed up the search. It divides the data into clusters (in this case, 10 clusters) and only searches the relevant clusters for the query. This reduces the number of comparisons needed, but the results are approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vZQci6lHutPJ"
   },
   "outputs": [],
   "source": [
    "# Train the IVF index and add data\n",
    "index_ivf.train(data)  # Training the IVF index on the dataset to create clusters\n",
    "index_ivf.add(data)  # Add data to the IVF index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Fe8VsEzutfF"
   },
   "source": [
    "Before we can add data to the `index_ivf`, we need to train it on the dataset. This step involves computing the centroids of 10 clusters. Itâ€™s required for clustering-based indices like IVF. After training, we can add the vectors to the index. The data is then distributed among the 10 clusters based on proximity to the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BJMoJeDzvO4P"
   },
   "outputs": [],
   "source": [
    "# Combine the two indices into an IndexShards\n",
    "sharded_index = faiss.IndexShards(dimension)  # Create an IndexShards object for combining indices\n",
    "sharded_index.add_shard(index_flat)  # Add the flat index as one shard\n",
    "sharded_index.add_shard(index_ivf)   # Add the IVF index as another shard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbuXTPM7vPHJ"
   },
   "source": [
    "`IndexShards` is a special type of index that allows us to combine multiple indices. Itâ€™s like a container that holds multiple indices and allows us to perform searches across all of them. By adding `index_flat` and `index_ivf` as **shards**, we are telling FAISS to perform searches using both indices together. This is useful when we want to combine the benefits of both types of indices.\n",
    "- The **flat index** will give us exact, brute-force searches for accuracy.\n",
    "- The **IVF index** will give us fast, approximate searches using clusters.\n",
    "\n",
    "We cannot combine indices of different types unless they are compatible. We must ensure that all indices being combined are compatible in terms of how data is stored and how the search is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYfzh-l0wCb6",
    "outputId": "3aa3ca6a-4ed4-492e-b642-3b1e9975d6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors: [[371 965 291 680 915]]\n",
      "Distances to nearest neighbors: [[16.6833   16.831635 16.843033 16.87417  17.035652]]\n"
     ]
    }
   ],
   "source": [
    "# Perform a search across both indices\n",
    "query_vector = np.random.random((1, dimension)).astype('float32')  # Generate a random query vector\n",
    "distances, indices = sharded_index.search(query_vector, 5)  # Perform a search across the combined indices\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Distances to nearest neighbors:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYmELTzWwC0M"
   },
   "source": [
    "We create a query vector (randomly generated in this case) and perform a search on the `sharded_index`. This will search across both the **Flat index** and the **IVF index**. The search returns the top-k nearest neighbors to the query vector. The `sharded_index.search()` will automatically query both indices and return the results from both.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
